abstract: We introduce a new language representation model called BERT, which stands
  for Bidirectional Encoder Representations from Transformers. Unlike recent language
  representation models, BERT is designed to pre-train deep bidirectional representations
  from unlabeled text by jointly conditioning on both left and right context in all
  layers. As a result, the pre-trained BERT model can be fine-tuned with just one
  additional output layer to create state-of-the-art models for a wide range of tasks,
  such as question answering and language inference, without substantial task-specific
  architecture modifications. BERT is conceptually simple and empirically powerful.
  It obtains new state-of-the-art results on eleven natural language processing tasks,
  including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
  accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test
  F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1
  point absolute improvement).
authors:
- affiliations:
  - aliases: []
    category: unknown
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5057457287
      type: openalex
    name: Jacob Devlin
  display_name: Jacob Devlin
- affiliations:
  - aliases: []
    category: unknown
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5076904467
      type: openalex
    - link: 0000-0002-0137-8895
      type: orcid
    name: Ming‐Wei Chang
  display_name: Ming‐Wei Chang
- affiliations:
  - aliases: []
    category: unknown
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5081862885
      type: openalex
    - link: 0000-0002-9534-5970
      type: orcid
    name: Kenton Lee
  display_name: Kenton Lee
- affiliations:
  - aliases: []
    category: unknown
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5053947885
      type: openalex
    name: Kristina Toutanova
  display_name: Kristina Toutanova
flags: []
links:
- link: W2896457183
  type: openalex
- link: 10.48550/arxiv.1810.04805
  type: doi
- link: '2896457183'
  type: mag
- link: https://arxiv.org/abs/1810.04805
  type: open-access
- link: https://arxiv.org/abs/1810.04805
  type: url
- link: https://api.datacite.org/dois/10.48550/arxiv.1810.04805
  type: url
releases:
- pages: null
  status: unknown
  venue:
    aliases: []
    date: '2018-01-01'
    date_precision: 3
    links:
    - link: https://arxiv.org/abs/1810.04805
      type: url
    name: arXiv (Cornell University)
    open: true
    peer_reviewed: false
    publisher: Cornell University
    series: ''
    type: unknown
    volume: null
title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
topics:
- name: Transformer
- name: Computer science
- name: Training (meteorology)
- name: Artificial intelligence
- name: Electrical engineering
- name: Engineering
- name: Geography
- name: Voltage
- name: Meteorology
