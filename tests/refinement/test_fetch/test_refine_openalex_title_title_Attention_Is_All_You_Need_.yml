abstract: The dominant sequence transduction models are based on complex recurrent
  or convolutional neural networks in an encoder-decoder configuration. The best performing
  models also connect the encoder and decoder through an attention mechanism. We propose
  a new simple network architecture, the Transformer, based solely on attention mechanisms,
  dispensing with recurrence and convolutions entirely. Experiments on two machine
  translation tasks show these models to be superior in quality while being more parallelizable
  and requiring significantly less time to train. Our model achieves 28.4 BLEU on
  the WMT 2014 English-to-German translation task, improving over the existing best
  results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation
  task, our model establishes a new single-model state-of-the-art BLEU score of 41.8
  after training for 3.5 days on eight GPUs, a small fraction of the training costs
  of the best models from the literature. We show that the Transformer generalizes
  well to other tasks by applying it successfully to English constituency parsing
  both with large and limited training data.
authors:
- affiliations:
  - aliases: []
    category: industry
    country: null
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5103024730
      type: openalex
    - link: 0000-0002-7794-2085
      type: orcid
    name: Ashish Vaswani
  display_name: Ashish Vaswani
- affiliations:
  - aliases: []
    category: industry
    country: null
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5021878400
      type: openalex
    name: Noam Shazeer
  display_name: Noam Shazeer
- affiliations:
  - aliases: []
    category: industry
    country: null
    name: Google (United States)
  - aliases: []
    category: academia
    country: null
    name: University of Southern California
  author:
    aliases: []
    links:
    - link: A5005777963
      type: openalex
    name: Niki Parmar
  display_name: Niki Parmar
- affiliations:
  - aliases: []
    category: industry
    country: null
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5022416424
      type: openalex
    - link: 0000-0001-5066-7530
      type: orcid
    name: Jakob Uszkoreit
  display_name: Jakob Uszkoreit
- affiliations:
  - aliases: []
    category: industry
    country: null
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5023448834
      type: openalex
    name: Llion Jones
  display_name: Llion Jones
- affiliations:
  - aliases: []
    category: academia
    country: null
    name: University of Toronto
  - aliases: []
    category: industry
    country: null
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5079288315
      type: openalex
    - link: 0000-0001-5601-5437
      type: orcid
    name: Aidan N. Gomez
  display_name: Aidan N. Gomez
- affiliations:
  - aliases: []
    category: industry
    country: null
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5031789995
      type: openalex
    - link: 0000-0003-1092-6010
      type: orcid
    name: Łukasz Kaiser
  display_name: Łukasz Kaiser
- affiliations:
  - aliases: []
    category: industry
    country: null
    name: Google (United States)
  author:
    aliases: []
    links:
    - link: A5045719436
      type: openalex
    name: Illia Polosukhin
  display_name: Illia Polosukhin
flags: []
links:
- link: '1706.03762'
  type: arxiv
- link: 10.48550/arxiv.1706.03762
  type: doi
- link: 10.65215/2q58a426
  type: doi
- link: 10.65215/ctdc8e75
  type: doi
- link: 10.65215/mdcm8z23
  type: doi
- link: 10.65215/ne77pf66
  type: doi
- link: 10.65215/nxvz2v36
  type: doi
- link: 10.65215/pc26a033
  type: doi
- link: 10.65215/r5bs2d54
  type: doi
- link: 10.65215/ysbyhc05
  type: doi
- link: '2626778328'
  type: mag
- link: https://langtaosha.org.cn/index.php/lts/preprint/download/10/108
  type: open-access
- link: W2626778328
  type: openalex
- link: https://langtaosha.org.cn/index.php/lts/preprint/download/10/108
  type: pdf
releases:
- pages: null
  status: preprint
  venue:
    aliases: []
    date: '2025-08-23'
    date_precision: 3
    links:
    - link: '1706.03762'
      type: arxiv
    - link: '1706.03762'
      type: arxiv
    name: arXiv (Cornell University)
    open: true
    peer_reviewed: false
    publisher: Cornell University
    series: ''
    short_name: null
    type: unknown
    volume: null
title: Attention Is All You Need
topics:
- name: Computer science
- name: Machine translation
- name: Transformer
- name: BLEU
- name: Encoder
- name: Artificial intelligence
- name: Parallelizable manifold
- name: Parsing
- name: Language model
- name: Natural language processing
- name: Decoding methods
- name: Task (project management)
- name: Convolutional neural network
- name: Speech recognition
- name: Machine learning
- name: Algorithm
- name: Voltage
- name: Physics
- name: Economics
- name: Management
- name: Quantum mechanics
- name: Operating system
