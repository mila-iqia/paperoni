__type__: Paper
abstract: Deep learning has advanced from fully connected architectures to structured
  models organized into components, e.g., the transformer composed of positional elements,
  modular architectures divided into slots, and graph neural nets made up of nodes.
  The nature of structured models is that communication among the components has a
  bottleneck, typically achieved by restricted connectivity and attention. In this
  work, we further tighten the bottleneck via discreteness of the representations
  transmitted between components. We hypothesize that this constraint serves as a
  useful form of inductive bias. Our hypothesis is motivated by past empirical work
  showing the benefits of discretization in non-structured architectures as well as
  our own theoretical results showing that discretization increases noise robustness
  and reduces the underlying dimensionality of the model. Building on an existing
  technique for discretization from the VQ-VAE, we consider multi-headed discretization
  with shared codebooks as the output of each architectural component. One motivating
  intuition is human language in which communication occurs through multiple discrete
  symbols. This form of communication is hypothesized to facilitate transmission of
  information between functional components of the brain by providing a common interlingua,
  just as it does for human-to-human communication. Our experiments show that discrete-valued
  neural communication (DVNC) substantially improves systematic generalization in
  a variety of architecturesâ€”transformers, modular architectures, and graph neural
  networks. We also show that the DVNC is robust to the choice of hyperparameters,
  making the method useful in practice.
authors:
- affiliations: []
  author:
    aliases: []
    links:
    - link: ~Dianbo_Liu2
      type: openreview
    name: Dianbo Liu
    quality:
    - 0.75
    roles: []
- affiliations: []
  author:
    aliases: []
    links:
    - link: ~Alex_Lamb1
      type: openreview
    name: Alex Lamb
    quality:
    - 0.75
    roles: []
- affiliations: []
  author:
    aliases: []
    links:
    - link: ~Kenji_Kawaguchi1
      type: openreview
    name: Kenji Kawaguchi
    quality:
    - 0.75
    roles: []
- affiliations: []
  author:
    aliases: []
    links:
    - link: ~Anirudh_Goyal1
      type: openreview
    name: Anirudh Goyal
    quality:
    - 0.75
    roles: []
- affiliations: []
  author:
    aliases: []
    links:
    - link: ~Chen_Sun7
      type: openreview
    name: Chen Sun
    quality:
    - 0.75
    roles: []
- affiliations: []
  author:
    aliases: []
    links:
    - link: ~Michael_Curtis_Mozer1
      type: openreview
    name: Michael Curtis Mozer
    quality:
    - 0.75
    roles: []
- affiliations: []
  author:
    aliases: []
    links:
    - link: ~Yoshua_Bengio1
      type: openreview
    name: Yoshua Bengio
    quality:
    - 0.75
    roles: []
citation_count: null
flags: []
links:
- link: YSYXmOzlrou
  type: openreview
quality:
- 0.0
releases:
- pages: null
  status: poster
  venue:
    aliases: []
    date: '2021-11-09T00:00:00'
    date_precision: 3
    links:
    - link: NeurIPS.cc/2021/Conference
      type: openreview-venue
    name: NeurIPS.cc/2021/Conference
    open: false
    peer_reviewed: false
    publisher: null
    quality:
    - 0.5
    series: NeurIPS.cc/Conference
    type: conference
    volume: NeurIPS 2021
title: Discrete-Valued Neural Communication
topics:
- name: structured architecture
- name: discretization
- name: communication
- name: specialist components
- name: system 2
