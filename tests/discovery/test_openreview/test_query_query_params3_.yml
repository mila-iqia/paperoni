- info: {}
  key: openreview:gVTkMsaaGI
  paper:
    abstract: 'Diffusion models have emerged as effective distribution estimators
      in vision, language, and reinforcement learning, but their use as priors in
      downstream tasks poses an intractable posterior inference problem. This paper
      studies *amortized* sampling of the posterior over data, $\mathbf{x}\sim p^{\rm
      post}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})$, in a model that consists
      of a diffusion generative model prior $p(\mathbf{x})$ and a black-box constraint
      or likelihood function $r(\mathbf{x})$. We state and prove the asymptotic correctness
      of a data-free learning objective, *relative trajectory balance*, for training
      a diffusion model that samples from this posterior, a problem that existing
      methods solve only approximately or in restricted cases. Relative trajectory
      balance arises from the generative flow network perspective on diffusion models,
      which allows the use of deep reinforcement learning techniques to improve mode
      coverage. Experiments illustrate the broad potential of unbiased inference of
      arbitrary posteriors under diffusion priors: in vision (classifier guidance),
      language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image
      generation). Beyond generative modeling, we apply relative trajectory balance
      to the problem of continuous control with a score-based behavior prior, achieving
      state-of-the-art results on benchmarks in offline reinforcement learning. Code
      is available at [this link](https://github.com/GFNOrg/diffusion-finetuning).'
    authors:
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Siddarth_Venkatraman1
          type: openreview
        name: Siddarth Venkatraman
      display_name: Siddarth Venkatraman
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Moksh_Jain1
          type: openreview
        name: Moksh Jain
      display_name: Moksh Jain
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Luca_Scimeca1
          type: openreview
        name: Luca Scimeca
      display_name: Luca Scimeca
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Minsu_Kim2
          type: openreview
        name: Minsu Kim
      display_name: Minsu Kim
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Marcin_Sendera1
          type: openreview
        name: Marcin Sendera
      display_name: Marcin Sendera
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Mohsin_Hasan1
          type: openreview
        name: Mohsin Hasan
      display_name: Mohsin Hasan
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Luke_Rowe1
          type: openreview
        name: Luke Rowe
      display_name: Luke Rowe
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Sarthak_Mittal1
          type: openreview
        name: Sarthak Mittal
      display_name: Sarthak Mittal
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Pablo_Lemos1
          type: openreview
        name: Pablo Lemos
      display_name: Pablo Lemos
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Emmanuel_Bengio1
          type: openreview
        name: Emmanuel Bengio
      display_name: Emmanuel Bengio
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Alexandre_Adam1
          type: openreview
        name: Alexandre Adam
      display_name: Alexandre Adam
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Jarrid_Rector-Brooks2
          type: openreview
        name: Jarrid Rector-Brooks
      display_name: Jarrid Rector-Brooks
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Yoshua_Bengio1
          type: openreview
        name: Yoshua Bengio
      display_name: Yoshua Bengio
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Glen_Berseth1
          type: openreview
        name: Glen Berseth
      display_name: Glen Berseth
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Nikolay_Malkin1
          type: openreview
        name: Nikolay Malkin
      display_name: Nikolay Malkin
    flags: []
    links:
    - link: gVTkMsaaGI
      type: openreview
    releases:
    - pages: null
      status: poster
      venue:
        aliases: []
        date: '2024-09-25'
        date_precision: 3
        links:
        - link: NeurIPS.cc/2024/Conference
          type: openreview-venue
        name: NeurIPS.cc/2024/Conference
        open: false
        peer_reviewed: false
        publisher: null
        series: NeurIPS.cc/Conference
        type: conference
        volume: NeurIPS 2024
    title: Amortizing intractable inference in diffusion models for vision, language,
      and control
    topics:
    - name: diffusion
    - name: inverse problems
    - name: conditional generation
    - name: language models
    - name: infilling
    - name: discrete diffusion
    - name: offline RL
    - name: planning
    - name: GFlowNet
  score: 0.0
- info: {}
  key: openreview:vieIamY2Gi
  paper:
    abstract: We study the problem of training diffusion models to sample from a distribution
      with a given unnormalized density or energy function. We benchmark several diffusion-structured
      inference methods, including simulation-based variational approaches and off-policy
      methods (continuous generative flow networks). Our results shed light on the
      relative advantages of existing algorithms while bringing into question some
      claims from past work. We also propose a novel exploration strategy for off-policy
      methods, based on local search in the target space with the use of a replay
      buffer, and show that it improves the quality of samples on a variety of target
      distributions. Our code for the sampling methods and benchmarks studied is made
      public at [this link](https://github.com/GFNOrg/gfn-diffusion) as a base for
      future work on diffusion models for amortized inference.
    authors:
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Marcin_Sendera1
          type: openreview
        name: Marcin Sendera
      display_name: Marcin Sendera
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Minsu_Kim2
          type: openreview
        name: Minsu Kim
      display_name: Minsu Kim
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Sarthak_Mittal1
          type: openreview
        name: Sarthak Mittal
      display_name: Sarthak Mittal
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Pablo_Lemos1
          type: openreview
        name: Pablo Lemos
      display_name: Pablo Lemos
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Luca_Scimeca1
          type: openreview
        name: Luca Scimeca
      display_name: Luca Scimeca
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Jarrid_Rector-Brooks2
          type: openreview
        name: Jarrid Rector-Brooks
      display_name: Jarrid Rector-Brooks
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Alexandre_Adam1
          type: openreview
        name: Alexandre Adam
      display_name: Alexandre Adam
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Yoshua_Bengio1
          type: openreview
        name: Yoshua Bengio
      display_name: Yoshua Bengio
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Nikolay_Malkin1
          type: openreview
        name: Nikolay Malkin
      display_name: Nikolay Malkin
    flags: []
    links:
    - link: vieIamY2Gi
      type: openreview
    releases:
    - pages: null
      status: poster
      venue:
        aliases: []
        date: '2024-09-25'
        date_precision: 3
        links:
        - link: NeurIPS.cc/2024/Conference
          type: openreview-venue
        name: NeurIPS.cc/2024/Conference
        open: false
        peer_reviewed: false
        publisher: null
        series: NeurIPS.cc/Conference
        type: conference
        volume: NeurIPS 2024
    title: Improved off-policy training of diffusion samplers
    topics:
    - name: diffusion models
    - name: amortized inference
    - name: stochastic control
    - name: GFlowNets
  score: 0.0
- info: {}
  key: openreview:D19UyP4HYk
  paper:
    abstract: '\emph{Metacognitive knowledge} refers to humans'' intuitive knowledge
      of their own thinking and reasoning processes. Today''s best LLMs clearly possess
      some reasoning processes. The paper gives evidence that they also  have metacognitive
      knowledge, including ability to name skills and procedures to apply given a
      task. We explore this primarily in context of math reasoning, developing a prompt-guided
      interaction procedure  to get a powerful  LLM to assign sensible skill labels
      to math questions, followed by having it perform semantic clustering to obtain
      coarser families of skill labels. These coarse skill labels look interpretable
      to humans.


      To validate that these skill labels are meaningful and relevant to the LLM''s
      reasoning processes we perform the following experiments. (a) We ask GPT-4 to
      assign skill labels to training questions in math datasets GSM8K and MATH.  (b)
      When using an LLM to solve the test questions, we present it with the full list
      of skill labels and ask it to identify the skill needed. Then it is presented
      with randomly selected exemplar solved questions associated with that skill
      label.  This improves accuracy on GSM8k and MATH for several strong LLMs, including
      code-assisted models. The methodology presented is domain-agnostic,  even though
      this article applies it to math problems.'
    authors:
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Aniket_Rajiv_Didolkar1
          type: openreview
        name: Aniket Rajiv Didolkar
      display_name: Aniket Rajiv Didolkar
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Anirudh_Goyal1
          type: openreview
        name: Anirudh Goyal
      display_name: Anirudh Goyal
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Nan_Rosemary_Ke1
          type: openreview
        name: Nan Rosemary Ke
      display_name: Nan Rosemary Ke
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Siyuan_Guo1
          type: openreview
        name: Siyuan Guo
      display_name: Siyuan Guo
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Michal_Valko1
          type: openreview
        name: Michal Valko
      display_name: Michal Valko
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Timothy_P_Lillicrap1
          type: openreview
        name: Timothy P Lillicrap
      display_name: Timothy P Lillicrap
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Danilo_Jimenez_Rezende2
          type: openreview
        name: Danilo Jimenez Rezende
      display_name: Danilo Jimenez Rezende
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Yoshua_Bengio1
          type: openreview
        name: Yoshua Bengio
      display_name: Yoshua Bengio
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Michael_Curtis_Mozer1
          type: openreview
        name: Michael Curtis Mozer
      display_name: Michael Curtis Mozer
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Sanjeev_Arora1
          type: openreview
        name: Sanjeev Arora
      display_name: Sanjeev Arora
    flags: []
    links:
    - link: D19UyP4HYk
      type: openreview
    releases:
    - pages: null
      status: poster
      venue:
        aliases: []
        date: '2024-09-25'
        date_precision: 3
        links:
        - link: NeurIPS.cc/2024/Conference
          type: openreview-venue
        name: NeurIPS.cc/2024/Conference
        open: false
        peer_reviewed: false
        publisher: null
        series: NeurIPS.cc/Conference
        type: conference
        volume: NeurIPS 2024
    title: 'Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem
      Solving'
    topics:
    - name: Metacognitive abilities of LLM
    - name: Mathematical Reasoning
  score: 0.0
- info: {}
  key: openreview:hpvJwmzEHX
  paper:
    abstract: Generative models hold great promise for small molecule discovery, significantly
      increasing the size of search space compared to traditional in silico screening
      libraries. However, most existing machine learning methods for small molecule
      generation suffer from poor synthesizability of candidate compounds, making
      experimental validation difficult. In this paper we propose Reaction-GFlowNet
      (RGFN), an extension of the GFlowNet framework that operates directly in the
      space of chemical reactions, thereby allowing out-of-the-box synthesizability
      while maintaining comparable quality of generated candidates. We demonstrate
      that with the proposed set of reactions and building blocks, it is possible
      to obtain a search space of molecules orders of magnitude larger than existing
      screening libraries coupled with low cost of synthesis. We also show that the
      approach scales to very large fragment libraries, further increasing the number
      of potential molecules. We demonstrate the effectiveness of the proposed approach
      across a range of oracle models, including pretrained proxy models and GPU-accelerated
      docking.
    authors:
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Michał_Koziarski1
          type: openreview
        name: Michał Koziarski
      display_name: Michał Koziarski
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Andrei_Rekesh1
          type: openreview
        name: Andrei Rekesh
      display_name: Andrei Rekesh
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Dmytro_Shevchuk1
          type: openreview
        name: Dmytro Shevchuk
      display_name: Dmytro Shevchuk
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Almer_M._van_der_Sloot1
          type: openreview
        name: Almer M. van der Sloot
      display_name: Almer M. van der Sloot
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Piotr_Gaiński1
          type: openreview
        name: Piotr Gaiński
      display_name: Piotr Gaiński
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Yoshua_Bengio1
          type: openreview
        name: Yoshua Bengio
      display_name: Yoshua Bengio
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Cheng-Hao_Liu1
          type: openreview
        name: Cheng-Hao Liu
      display_name: Cheng-Hao Liu
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Mike_Tyers1
          type: openreview
        name: Mike Tyers
      display_name: Mike Tyers
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Robert_A._Batey1
          type: openreview
        name: Robert A. Batey
      display_name: Robert A. Batey
    flags: []
    links:
    - link: hpvJwmzEHX
      type: openreview
    releases:
    - pages: null
      status: poster
      venue:
        aliases: []
        date: '2024-09-25'
        date_precision: 3
        links:
        - link: NeurIPS.cc/2024/Conference
          type: openreview-venue
        name: NeurIPS.cc/2024/Conference
        open: false
        peer_reviewed: false
        publisher: null
        series: NeurIPS.cc/Conference
        type: conference
        volume: NeurIPS 2024
    title: 'RGFN: Synthesizable Molecular Generation Using GFlowNets'
    topics:
    - name: drug discovery
    - name: generative models
    - name: GFlowNets
    - name: synthesizability
  score: 0.0
- info: {}
  key: openreview:fNakQltI1N
  paper:
    abstract: "Modeling stochastic and irregularly sampled time series is a challenging\
      \ problem found in a wide range of applications, especially in medicine. Neural\
      \ stochastic differential equations (Neural SDEs) are an attractive modeling\
      \ technique for this problem, which parameterize the drift and diffusion terms\
      \ of an SDE with neural networks. However, current algorithms for training Neural\
      \ SDEs require backpropagation through the SDE dynamics, greatly limiting their\
      \ scalability and stability. \nTo address this, we propose **Trajectory Flow\
      \ Matching** (TFM), which trains a Neural SDE in a *simulation-free* manner,\
      \ bypassing backpropagation through the dynamics. TFM leverages the flow matching\
      \ technique from generative modeling to model time series. In this work we first\
      \ establish necessary conditions for TFM to learn time series data. Next, we\
      \ present a reparameterization trick which improves training stability. Finally,\
      \ we adapt TFM to the clinical time series setting, demonstrating improved performance\
      \ on four clinical time series datasets both in terms of absolute performance\
      \ and uncertainty prediction, a crucial parameter in this setting."
    authors:
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Xi_Zhang18
          type: openreview
        name: Xi Zhang
      display_name: Xi Zhang
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Yuan_Pu4
          type: openreview
        name: Yuan Pu
      display_name: Yuan Pu
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Yuki_Kawamura1
          type: openreview
        name: Yuki Kawamura
      display_name: Yuki Kawamura
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Andrew_Loza1
          type: openreview
        name: Andrew Loza
      display_name: Andrew Loza
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Yoshua_Bengio1
          type: openreview
        name: Yoshua Bengio
      display_name: Yoshua Bengio
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Dennis_Shung1
          type: openreview
        name: Dennis Shung
      display_name: Dennis Shung
    - affiliations: []
      author:
        aliases: []
        links:
        - link: ~Alexander_Tong1
          type: openreview
        name: Alexander Tong
      display_name: Alexander Tong
    flags: []
    links:
    - link: fNakQltI1N
      type: openreview
    releases:
    - pages: null
      status: spotlight
      venue:
        aliases: []
        date: '2024-09-25'
        date_precision: 3
        links:
        - link: NeurIPS.cc/2024/Conference
          type: openreview-venue
        name: NeurIPS.cc/2024/Conference
        open: false
        peer_reviewed: false
        publisher: null
        series: NeurIPS.cc/Conference
        type: conference
        volume: NeurIPS 2024
    title: Trajectory Flow Matching with Applications to Clinical Time Series Modelling
    topics:
    - name: Flow matching
    - name: stochastic differential equations
    - name: ODE
    - name: SDE
    - name: uncertainty
    - name: time series
    - name: EHR
  score: 0.0
