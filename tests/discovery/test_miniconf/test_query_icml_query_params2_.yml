- info: {}
  key: miniconf:icml:989652eef28bc49eec908063ba36a854
  paper:
    abstract: The dynamical formulation of the optimal transport can be extended through
      various choices of the underlying geometry (*kinetic energy*), and the regularization
      of density paths (*potential energy*). These combinations yield different variational
      problems (*Lagrangians*), encompassing many variations of the optimal transport
      problem such as the Schrödinger bridge, unbalanced optimal transport, and optimal
      transport with physical constraints, among others. In general, the optimal density
      path is unknown, and solving these variational problems can be computationally
      challenging. We propose a novel deep learning based framework approaching all
      of these problems from a unified perspective. Leveraging the dual formulation
      of the Lagrangians, our method does not require simulating or backpropagating
      through the trajectories of the learned dynamics, and does not need access to
      optimal couplings. We showcase the versatility of the proposed framework by
      outperforming previous approaches for the single-cell trajectory inference,
      where incorporating prior knowledge into the dynamics is crucial for correct
      predictions.
    authors:
    - affiliations:
      - aliases: []
        category: unknown
        name: Université de Montréal, Mila
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/8271?format=json
          type: profile
        name: Kirill Neklyudov
      display_name: Kirill Neklyudov
    - affiliations:
      - aliases: []
        category: unknown
        name: Vector Institute
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/88580?format=json
          type: profile
        name: Rob Brekelmans
      display_name: Rob Brekelmans
    - affiliations:
      - aliases: []
        category: unknown
        name: Mila
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/92402?format=json
          type: profile
        name: Alexander Tong
      display_name: Alexander Tong
    - affiliations:
      - aliases: []
        category: unknown
        name: University of Toronto Vector Institute
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/97996?format=json
          type: profile
        name: Lazar Atanackovic
      display_name: Lazar Atanackovic
    - affiliations:
      - aliases: []
        category: unknown
        name: University of Texas, Austin
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/92899?format=json
          type: profile
        name: qiang liu
      display_name: qiang liu
    - affiliations:
      - aliases: []
        category: unknown
        name: University of Toronto
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/37429?format=json
          type: profile
        name: Alireza Makhzani
      display_name: Alireza Makhzani
    flags: []
    links:
    - link: https://icml.cc/virtual/2024/poster/32732
      type: abstract
    - link: 235/neklyudov24a
      type: mlr
    - link: 989652eef28bc49eec908063ba36a854
      type: uid
    releases:
    - pages: null
      status: Accept (Poster)
      venue:
        aliases: []
        date: '2024-07-23'
        date_precision: 3
        links: []
        name: ICML
        open: true
        peer_reviewed: true
        publisher: null
        series: ICML
        type: conference
        volume: null
    title: A Computational Framework for Solving Wasserstein Lagrangian Flows
    topics:
    - name: Probabilistic Methods
  score: 0.0
- info: {}
  key: miniconf:icml:a6155b0da06d1ad154ad2d039d1fadf4
  paper:
    abstract: This paper contributes a new approach for distributional reinforcement
      learning which elucidates a clean separation of transition structure and reward
      in the learning process. Analogous to how the successor representation (SR)
      describes the expected consequences of behaving according to a given policy,
      our distributional successor measure (SM) describes the distributional consequences
      of this behaviour. We formulate the distributional SM as a distribution over
      distributions and provide theory connecting it with distributional and model-based
      reinforcement learning. Moreover, we propose an algorithm that learns the distributional
      SM from data by minimizing a two-level maximum mean discrepancy. Key to our
      method are a number of algorithmic techniques that are independently valuable
      for learning generative models of state. As an illustration of the usefulness
      of the distributional SM, we show that it enables zero-shot risk-sensitive policy
      evaluation in a way that was not previously possible.
    authors:
    - affiliations:
      - aliases: []
        category: unknown
        name: McGill University, Mila
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/72783?format=json
          type: profile
        name: Harley Wiltzer
      display_name: Harley Wiltzer
    - affiliations:
      - aliases: []
        category: unknown
        name: Mila / McGill
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/121983?format=json
          type: profile
        name: Jesse Farebrother
      display_name: Jesse Farebrother
    - affiliations:
      - aliases: []
        category: unknown
        name: Gatsby Unit and Google Deepmind
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/8312?format=json
          type: profile
        name: Arthur Gretton
      display_name: Arthur Gretton
    - affiliations:
      - aliases: []
        category: unknown
        name: Google DeepMind
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/15946?format=json
          type: profile
        name: Yunhao Tang
      display_name: Yunhao Tang
    - affiliations:
      - aliases: []
        category: unknown
        name: DeepMind
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/8841?format=json
          type: profile
        name: Andre Barreto
      display_name: Andre Barreto
    - affiliations:
      - aliases: []
        category: unknown
        name: Google DeepMind
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/8541?format=json
          type: profile
        name: Will Dabney
      display_name: Will Dabney
    - affiliations:
      - aliases: []
        category: unknown
        name: Google DeepMind
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/7528?format=json
          type: profile
        name: Marc Bellemare
      display_name: Marc Bellemare
    - affiliations:
      - aliases: []
        category: unknown
        name: Google DeepMind
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/23745?format=json
          type: profile
        name: Mark Rowland
      display_name: Mark Rowland
    flags: []
    links:
    - link: https://icml.cc/virtual/2024/poster/32627
      type: abstract
    - link: 235/wiltzer24a
      type: mlr
    - link: a6155b0da06d1ad154ad2d039d1fadf4
      type: uid
    releases:
    - pages: null
      status: Accept (Spotlight)
      venue:
        aliases: []
        date: '2024-07-23'
        date_precision: 3
        links: []
        name: ICML
        open: true
        peer_reviewed: true
        publisher: null
        series: ICML
        type: conference
        volume: null
    title: A Distributional Analogue to the Successor Representation
    topics:
    - name: Reinforcement Learning->Everything Else
  score: 0.0
- info: {}
  key: miniconf:icml:285da2198b2b496c9d447cc4ac6b0734
  paper:
    abstract: Bayesian Persuasion is proposed as a tool for social media platforms
      to combat the spread of misinformation. Since platforms can use machine learning
      to predict the popularity and misinformation features of to-be-shared posts,
      and users are largely motivated to share popular content, platforms can strategically
      signal this informational advantage to change user beliefs and persuade them
      not to share misinformation. We characterize the optimal signaling scheme with
      imperfect predictions as a linear program and give sufficient and necessary
      conditions on the classifier to ensure optimal platform utility is non-decreasing
      and continuous. Next, this interaction is considered under a performative model,
      wherein platform intervention affects the user's future behaviour. The convergence
      and stability of optimal signaling under this performative process are fully
      characterized. Lastly, we experimentally validate that our approach significantly
      reduces misinformation in both the single round and performative setting.
    authors:
    - affiliations:
      - aliases: []
        category: unknown
        name: Harvard University
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/124592?format=json
          type: profile
        name: Safwan Hossain
      display_name: Safwan Hossain
    - affiliations:
      - aliases: []
        category: unknown
        name: Montreal Institute for Learning Algorithms, University of Montreal,
          University of Montreal
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/124566?format=json
          type: profile
        name: Andjela Mladenovic
      display_name: Andjela Mladenovic
    - affiliations:
      - aliases: []
        category: unknown
        name: Harvard University
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/19475?format=json
          type: profile
        name: Yiling Chen
      display_name: Yiling Chen
    - affiliations:
      - aliases: []
        category: unknown
        name: Mila
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/73156?format=json
          type: profile
        name: Gauthier Gidel
      display_name: Gauthier Gidel
    flags: []
    links:
    - link: https://icml.cc/virtual/2024/poster/33949
      type: abstract
    - link: 235/hossain24b
      type: mlr
    - link: https://icml.cc/media/icml-2024/Slides/33949_ypaq1v1.pdf
      type: slides
    - link: 285da2198b2b496c9d447cc4ac6b0734
      type: uid
    releases:
    - pages: null
      status: Accept (Poster)
      venue:
        aliases: []
        date: '2024-07-23'
        date_precision: 3
        links: []
        name: ICML
        open: true
        peer_reviewed: true
        publisher: null
        series: ICML
        type: conference
        volume: null
    title: A Persuasive Approach to Combating Misinformation
    topics: []
  score: 0.0
- info: {}
  key: miniconf:icml:d465f14a648b3d0a1faa6f447e526c60
  paper:
    abstract: 'Causal representation learning aims at identifying high-level causal
      variables from perceptual data. Most methods assume that all latent causal variables
      are captured in the high-dimensional observations. We instead consider a partially
      observed setting, in which each measurement only provides information about
      a subset of the underlying causal state. Prior work has studied this setting
      with multiple domains or views, each depending on a fixed subset of latents.
      Here, we focus on learning from unpaired observations from a dataset with an
      instance-dependent partial observability pattern. Our main contribution is to
      establish two identifiability results for this setting: one for linear mixing
      functions without parametric assumptions on the underlying causal model, and
      one for piecewise linear mixing functions with Gaussian latent causal variables.
      Based on these insights, we propose two methods for estimating the underlying
      causal variables by enforcing sparsity in the inferred representation. Experiments
      on different simulated datasets and established benchmarks highlight the effectiveness
      of our approach in recovering the ground-truth latents.'
    authors:
    - affiliations:
      - aliases: []
        category: unknown
        name: University of Amsterdam
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/125181?format=json
          type: profile
        name: Danru Xu
      display_name: Danru Xu
    - affiliations:
      - aliases: []
        category: unknown
        name: ISTA
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/128685?format=json
          type: profile
        name: Dingling Yao
      display_name: Dingling Yao
    - affiliations:
      - aliases: []
        category: unknown
        name: Université de Montréal, Mila
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/62189?format=json
          type: profile
        name: Sébastien Lachapelle
      display_name: Sébastien Lachapelle
    - affiliations:
      - aliases: []
        category: unknown
        name: Service Now
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/62187?format=json
          type: profile
        name: Perouz Taslakian
      display_name: Perouz Taslakian
    - affiliations:
      - aliases: []
        category: unknown
        name: MPI for Intelligent Systems, Tübingen & University of Cambridge
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/58738?format=json
          type: profile
        name: Julius von Kügelgen
      display_name: Julius von Kügelgen
    - affiliations:
      - aliases: []
        category: unknown
        name: ISTA
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/103725?format=json
          type: profile
        name: Francesco Locatello
      display_name: Francesco Locatello
    - affiliations:
      - aliases: []
        category: unknown
        name: University of Amsterdam, MIT-IBM Watson AI Lab
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/73085?format=json
          type: profile
        name: Sara Magliacane
      display_name: Sara Magliacane
    flags: []
    links:
    - link: https://icml.cc/virtual/2024/poster/34245
      type: abstract
    - link: 235/xu24ac
      type: mlr
    - link: d465f14a648b3d0a1faa6f447e526c60
      type: uid
    releases:
    - pages: null
      status: Accept (Poster)
      venue:
        aliases: []
        date: '2024-07-23'
        date_precision: 3
        links: []
        name: ICML
        open: true
        peer_reviewed: true
        publisher: null
        series: ICML
        type: conference
        volume: null
    title: A Sparsity Principle for Partially Observable Causal Representation Learning
    topics:
    - name: Miscellaneous Aspects of Machine Learning->Causality
  score: 0.0
- info: {}
  key: miniconf:icml:f2c5b1f06bfe59954cb2a56858c2ed98
  paper:
    abstract: Second-order Recurrent Neural Networks (2RNNs) extend RNNs by leveraging
      second-order interactions for sequence modelling. These models are provably
      more expressive than their first-order counterparts and have connections to
      well-studied models from formal language theory. However, their large parameter
      tensor makes computations intractable. To circumvent this issue, one approach
      known as MIRNN consists in limiting the type of interactions used by the model.
      Another is to leverage tensor decomposition to diminish the parameter count.
      In this work, we study the model resulting from parameterizing 2RNNs using the
      CP decomposition, which we call CPRNN. Intuitively, the rank of the decomposition
      should reduce expressivity. We analyze how rank and hidden size affect model
      capacity and show the relationships between RNNs, 2RNNs, MIRNNs, and CPRNNs
      based on these parameters. We support these results empirically with experiments
      on the Penn Treebank dataset which demonstrate that, with a fixed parameter
      budget, CPRNNs outperforms RNNs, 2RNNs, and MIRNNs with the right choice of
      rank and hidden size.
    authors:
    - affiliations:
      - aliases: []
        category: unknown
        name: Mila - Université de Montréal
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/125764?format=json
          type: profile
        name: Maude Lizaire
      display_name: Maude Lizaire
    - affiliations:
      - aliases: []
        category: unknown
        name: Université de Montréal
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/125766?format=json
          type: profile
        name: Michael Rizvi-Martel
      display_name: Michael Rizvi-Martel
    - affiliations: []
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/106509?format=json
          type: profile
        name: Marawan Gamal
      display_name: Marawan Gamal
    - affiliations:
      - aliases: []
        category: unknown
        name: Montreal Institute for Learning Algorithms, University of Montreal,
          University of Montreal
      author:
        aliases: []
        links:
        - link: http://icml.cc/api/miniconf/users/99756?format=json
          type: profile
        name: Guillaume Rabusseau
      display_name: Guillaume Rabusseau
    flags: []
    links:
    - link: https://icml.cc/virtual/2024/poster/34560
      type: abstract
    - link: 235/lizaire24a
      type: mlr
    - link: f2c5b1f06bfe59954cb2a56858c2ed98
      type: uid
    releases:
    - pages: null
      status: Accept (Spotlight)
      venue:
        aliases: []
        date: '2024-07-23'
        date_precision: 3
        links: []
        name: ICML
        open: true
        peer_reviewed: true
        publisher: null
        series: ICML
        type: conference
        volume: null
    title: A Tensor Decomposition Perspective on Second-order RNNs
    topics:
    - name: Deep Learning->Sequential Models, Time series
  score: 0.0
