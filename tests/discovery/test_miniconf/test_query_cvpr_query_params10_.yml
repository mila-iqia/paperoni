"[\n  {\n    \"abstract\": \"Foundation models encompass an extensive knowledge base\
  \ and offer remarkable transferability. However, this knowledge becomes outdated\
  \ or insufficient over time. The challenge lies in continuously updating foundation\
  \ models to accommodate novel information while retaining their original capabilities.\
  \  Leveraging the fact that foundation models have initial knowledge on various\
  \ tasks and domains, we propose a novel approach that,  instead of updating all\
  \ parameters equally,  localizes the updates to a sparse set of parameters relevant\
  \ to the task being learned. We strike a  balance between efficiency and new task\
  \ performance, while maintaining the transferability and generalizability of foundation\
  \ models. We extensively evaluate our method on foundational vision-language models\
  \ with a diverse spectrum of continual learning tasks. Our method achieves improvements\
  \ on the accuracy of the newly learned tasks up to 7% while preserving the pretraining\
  \ knowledge with a negligible decrease of 0.9% on a representative control set accuracy.\"\
  ,\n    \"authors\": [\n      {\n        \"affiliations\": [\n          {\n     \
  \       \"aliases\": [],\n            \"category\": \"unknown\",\n            \"\
  name\": \"Concordia University/ MILA\"\n          }\n        ],\n        \"author\"\
  : {\n          \"aliases\": [],\n          \"links\": [\n            {\n       \
  \       \"link\": \"http://cvpr.thecvf.com/api/miniconf/users/131766?format=json\"\
  ,\n              \"type\": \"profile\"\n            }\n          ],\n          \"\
  name\": \"Paul Janson\"\n        },\n        \"display_name\": \"Paul Janson\"\n\
  \      },\n      {\n        \"affiliations\": [\n          {\n            \"aliases\"\
  : [],\n            \"category\": \"unknown\",\n            \"name\": \"KAUST\"\n\
  \          }\n        ],\n        \"author\": {\n          \"aliases\": [],\n  \
  \        \"links\": [\n            {\n              \"link\": \"http://cvpr.thecvf.com/api/miniconf/users/85585?format=json\"\
  ,\n              \"type\": \"profile\"\n            }\n          ],\n          \"\
  name\": \"Mohamed Elhoseiny\"\n        },\n        \"display_name\": \"Mohamed Elhoseiny\"\
  \n      },\n      {\n        \"affiliations\": [\n          {\n            \"aliases\"\
  : [],\n            \"category\": \"unknown\",\n            \"name\": \"King Abdullah\
  \ University of Science and Technology\"\n          }\n        ],\n        \"author\"\
  : {\n          \"aliases\": [],\n          \"links\": [\n            {\n       \
  \       \"link\": \"http://cvpr.thecvf.com/api/miniconf/users/131776?format=json\"\
  ,\n              \"type\": \"profile\"\n            }\n          ],\n          \"\
  name\": \"Wenxuan Zhang\"\n        },\n        \"display_name\": \"Wenxuan Zhang\"\
  \n      },\n      {\n        \"affiliations\": [\n          {\n            \"aliases\"\
  : [],\n            \"category\": \"unknown\",\n            \"name\": \"Toyota Motor\
  \ Europe\"\n          }\n        ],\n        \"author\": {\n          \"aliases\"\
  : [],\n          \"links\": [\n            {\n              \"link\": \"http://cvpr.thecvf.com/api/miniconf/users/75692?format=json\"\
  ,\n              \"type\": \"profile\"\n            }\n          ],\n          \"\
  name\": \"Rahaf Aljundi\"\n        },\n        \"display_name\": \"Rahaf Aljundi\"\
  \n      }\n    ],\n    \"flags\": [],\n    \"links\": [\n      {\n        \"link\"\
  : \"https://cvpr.thecvf.com/virtual/2024/poster/31810\",\n        \"type\": \"abstract\"\
  \n      },\n      {\n        \"link\": \"https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Overcoming_Generic_Knowledge_Loss_with_Selective_Parameter_Update_CVPR_2024_paper.html\"\
  ,\n        \"type\": \"abstract\"\n      },\n      {\n        \"link\": \"https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Overcoming_Generic_Knowledge_Loss_with_Selective_Parameter_Update_CVPR_2024_paper.html\"\
  ,\n        \"type\": \"paper_pdf\"\n      },\n      {\n        \"link\": \"https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Overcoming_Generic_Knowledge_Loss_with_Selective_Parameter_Update_CVPR_2024_paper.html\"\
  ,\n        \"type\": \"pdf\"\n      }\n    ],\n    \"releases\": [\n      {\n  \
  \      \"pages\": null,\n        \"status\": \"Accept: Poster\",\n        \"venue\"\
  : {\n          \"aliases\": [],\n          \"date\": \"2024-06-19\",\n         \
  \ \"date_precision\": 3,\n          \"links\": [],\n          \"name\": \"CVPR\"\
  ,\n          \"open\": true,\n          \"peer_reviewed\": true,\n          \"publisher\"\
  : null,\n          \"series\": \"CVPR\",\n          \"type\": \"conference\",\n\
  \          \"volume\": null\n        }\n      }\n    ],\n    \"title\": \"Overcoming\
  \ Generic Knowledge Loss with Selective Parameter Update\",\n    \"topics\": []\n\
  \  },\n  {\n    \"abstract\": \"Vision-Language Models (VLMs), such as CLIP, exhibit\
  \ strong image-text comprehension abilities, facilitating advances in several downstream\
  \ tasks such as zero-shot image classification, image-text retrieval, and text-to-image\
  \ generation. However, the compositional reasoning abilities of existing VLMs remains\
  \ subpar. The root of this limitation lies in the inadequate alignment between the\
  \ images and captions in the pretraining datasets. Additionally, the current contrastive\
  \ learning objective fails to focus on fine-grained grounding components like relations,\
  \ actions, and attributes, resulting in \\\"bag-of-words\\\" representations. We\
  \ introduce a simple and effective method to improve compositional reasoning in\
  \ VLMs. Our method better leverages available datasets by refining and expanding\
  \ the standard image-text contrastive learning framework. Our approach does not\
  \ require specific annotations and does not incur extra parameters. When integrated\
  \ with CLIP, our technique yields notable improvement over state-of-the-art baselines\
  \ across five vision-language compositional benchmarks.\",\n    \"authors\": [\n\
  \      {\n        \"affiliations\": [],\n        \"author\": {\n          \"aliases\"\
  : [],\n          \"links\": [\n            {\n              \"link\": \"http://cvpr.thecvf.com/api/miniconf/users/92323?format=json\"\
  ,\n              \"type\": \"profile\"\n            }\n          ],\n          \"\
  name\": \"Aishwarya Agrawal\"\n        },\n        \"display_name\": \"Aishwarya\
  \ Agrawal\"\n      },\n      {\n        \"affiliations\": [],\n        \"author\"\
  : {\n          \"aliases\": [],\n          \"links\": [\n            {\n       \
  \       \"link\": \"http://cvpr.thecvf.com/api/miniconf/users/94650?format=json\"\
  ,\n              \"type\": \"profile\"\n            }\n          ],\n          \"\
  name\": \"Rabiul Awal\"\n        },\n        \"display_name\": \"Rabiul Awal\"\n\
  \      },\n      {\n        \"affiliations\": [\n          {\n            \"aliases\"\
  : [],\n            \"category\": \"unknown\",\n            \"name\": \"Mila-Quebec\
  \ AI Institute\"\n          }\n        ],\n        \"author\": {\n          \"aliases\"\
  : [],\n          \"links\": [\n            {\n              \"link\": \"http://cvpr.thecvf.com/api/miniconf/users/127449?format=json\"\
  ,\n              \"type\": \"profile\"\n            }\n          ],\n          \"\
  name\": \"Le Zhang\"\n        },\n        \"display_name\": \"Le Zhang\"\n     \
  \ }\n    ],\n    \"flags\": [],\n    \"links\": [\n      {\n        \"link\": \"\
  https://cvpr.thecvf.com/virtual/2024/poster/29738\",\n        \"type\": \"abstract\"\
  \n      },\n      {\n        \"link\": \"https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Contrasting_Intra-Modal_and_Ranking_Cross-Modal_Hard_Negatives_to_Enhance_Visio-Linguistic_CVPR_2024_paper.html\"\
  ,\n        \"type\": \"abstract\"\n      },\n      {\n        \"link\": \"https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Contrasting_Intra-Modal_and_Ranking_Cross-Modal_Hard_Negatives_to_Enhance_Visio-Linguistic_CVPR_2024_paper.html\"\
  ,\n        \"type\": \"paper_pdf\"\n      },\n      {\n        \"link\": \"https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Contrasting_Intra-Modal_and_Ranking_Cross-Modal_Hard_Negatives_to_Enhance_Visio-Linguistic_CVPR_2024_paper.html\"\
  ,\n        \"type\": \"pdf\"\n      }\n    ],\n    \"releases\": [\n      {\n  \
  \      \"pages\": null,\n        \"status\": \"Accept: Poster\",\n        \"venue\"\
  : {\n          \"aliases\": [],\n          \"date\": \"2024-06-19\",\n         \
  \ \"date_precision\": 3,\n          \"links\": [],\n          \"name\": \"CVPR\"\
  ,\n          \"open\": true,\n          \"peer_reviewed\": true,\n          \"publisher\"\
  : null,\n          \"series\": \"CVPR\",\n          \"type\": \"conference\",\n\
  \          \"volume\": null\n        }\n      }\n    ],\n    \"title\": \"Contrasting\
  \ Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic\
  \ Compositional Understanding\",\n    \"topics\": []\n  }\n]"
